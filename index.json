[{"authors":["admin"],"categories":null,"content":"Hi! I am developing data-driven approaches to smart charging of batteries and supercapacitors. My work spans the fields of machine learning, physics and chemistry.\n","date":1605484800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1605484800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://penelopejones.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi! I am developing data-driven approaches to smart charging of batteries and supercapacitors. My work spans the fields of machine learning, physics and chemistry.","tags":null,"title":"Penelope Jones","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"01d39688af48040d7111d20c6b01d10b","permalink":"https://penelopejones.github.io/courses/example.1/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example.1/","section":"courses","summary":"Learn how to use Academia's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview 2","type":"docs"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"970b63766f241ba034ef2d1838dad4b3","permalink":"https://penelopejones.github.io/courses/example.2/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example.2/","section":"courses","summary":"Learn how to use Academia's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview 3","type":"docs"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://penelopejones.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academia's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academia:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://penelopejones.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academia:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academia:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"883c5cf2007b50b720ce5f4a17b72261","permalink":"https://penelopejones.github.io/courses/example.1/example3/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example.1/example3/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academia:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academia:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"32df3afea9008eacfc678398fd6443ad","permalink":"https://penelopejones.github.io/courses/example.2/example3/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example.2/example3/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academia:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with academia:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"ae73c2dafb9eed90c8870cdf783e948c","permalink":"https://penelopejones.github.io/courses/example.2/example4/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example.2/example4/","section":"courses","summary":"Here are some more tips for getting started with academia:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with academia:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://penelopejones.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with academia:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with academia:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"507e4cd2746d8b7ea0b6ed4a42cb4462","permalink":"https://penelopejones.github.io/courses/example.1/example4/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example.1/example4/","section":"courses","summary":"Here are some more tips for getting started with academia:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using academia\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"8ebbbdeba622fbc24dab29b4efa6b173","permalink":"https://penelopejones.github.io/talk/coding/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/coding/","section":"talk","summary":"An example talk using academia's Markdown slides feature.","tags":[],"title":"Coding and Analyzing Qualitative Data","type":"talk"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using academia\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a3975af5f5dadc9b2c7bbc4e48bb0e6e","permalink":"https://penelopejones.github.io/talk/qualitative/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/qualitative/","section":"talk","summary":"An example talk using academia's Markdown slides feature.","tags":[],"title":"Qualitative Research Summer Intensive","type":"talk"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using academia\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"f917008f74aa8012979052dcf8dbf864","permalink":"https://penelopejones.github.io/talk/synthesizing/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/synthesizing/","section":"talk","summary":"An example talk using academia's Markdown slides feature.","tags":[],"title":"Synthesizing Qualitative Data","type":"talk"},{"authors":["Penelope Jones"],"categories":[],"content":"An assortment of physical systems exist that can be considered stochastic processes. For example, diffusion and heat transfer can be modelled as Brownian processes, and the number of decays from a radioactive sample can be modelled as a Poisson process. In daily life, social network dynamics, traffic flow and stock market prices are routinely modelled using the mathematics of stochastic processes [1].\nA stochastic process, $\\mathcal{P}$, defines a family of random variables, which may have a finite or infinite number of components. Each realisation of a stochastic process ${y_x, x \\in \\mathcal{X}}$ is an assignment of a possible value of $y_x$ to each $x \\in \\mathcal{X}$ (Karlin, 1968). When the domain $\\mathcal{X}$ is real-valued, such a realisation corresponds to a function, $f$, mapping $x$ to a corresponding value $y_x$. 1 Thus we can interpret a stochastic process as defining a distribution over functions, with each sample from $\\mathcal{P}$ corresponding to a different function.\nWe can completely characterise a stochastic process by specifying the joint distribution of every finite family $y_{x_1}, y_{x_2}, \u0026hellip; , y_{x_n}$ of variables. 2 By placing restrictions on the form of the joint distribution, inference can be made tractable. One such example of this is the Gaussian process, a widely used stochastic process that has found success in a variety of domains.\nGaussian processes A Gaussian process (GP) is a stochastic process for which the joint distribution of any finite subset of the set of random variables is Gaussian. GPs are used extensively in a range of applications, including in Bayesian optimisation, reinforcement learning and property prediction.\nA GP is wholly specified by a mean function $m(x)$ and covariance function $k(x, x\u0026rsquo;)$, where for $\\mathit{f}\\sim\\mathcal{P}$: $$\\begin{aligned} m(x) \u0026amp;= \\mathbb{E}[f(x)] \\\\\nk(x, x\u0026rsquo;) \u0026amp;= \\mathbb{E}[(f(x) - m(x))(f(x\u0026rsquo;) - m(x\u0026rsquo;))].\\end{aligned}$$ Often $m(x)$ is set to be 0. The covariance function specifies the covariance between the random variables $f(x)$ and $f(x\u0026rsquo;)$. For commonly used examples such as the squared exponential and Matrn covariance functions, the covariance is close to unity when inputs $x$ and $x'$ are in close proximity, but reduces to zero as their separation distance increases.\nThe choice of mean and covariance function dictates the properties of the sampled functions, such as smoothness, lengthscale and periodicity. Through these choices, the inductive biases are specified. For example, consider the squared exponential kernel: $$k(x, x\u0026rsquo;) = \\exp{\\left(-\\frac{|x - x'|^2}{2l^2} \\right)}.$$\nHere, the hyperparameter $l$ is a measure of the lengthscale in input space over which random variables are no longer significantly correlated. In practice, the prior hyperparameters are learnt through maximisation of the marginal likelihood $p(X|l)$ (Rasmussen and Williams, 2006), as outlined in my previous post.\nLet us now consider that we have made some noisy observations, $y= { y_i } _{i=1}^m$ at locations $X={x_i}_{i=1}^m$. We will assume that these observations are our window for gaining insights about the underlying function $f$ from which the observations were made. We want to predict the value of the function at an additional number of locations, $X^{*} = {x_i}_{i=m+1}^n$.\nAssuming that observations are corrupted by Gaussian noise with variance $\\sigma^2$, the joint distribution of the observed values $y$ and the test values $f_{*}$ is $$ \\left[\\begin{matrix} \\mathbf{y} \\\\ \\mathbf{f}_{*} \\end{matrix}\\right] \\sim \\mathcal{N}\\left(\\mathbf{0},\\left[\\begin{matrix} k(X, X)+\\sigma^{2} I \u0026amp; k(X, X_{*}) \\\\\nk\\left(X_{*}, X\\right) \u0026amp; k\\left(X_{*}, X_{*}\\right) \\end{matrix}\\right]\\right).$$\nConditioning on the observed values, we obtain the posterior distribution over $f_{*}$:\n$$ f_{*} | X, y, X_{*} \\sim \\mathcal{N}(\\bar{f_{*}}, \\mathrm{cov} \\left(f_{*}\\right)) $$\nwhere $$ \\bar{f_{*}} = k\\left(X_{*}, X\\right)\\left[k\\left(X, X\\right)+\\sigma^2I \\right]^{-1} y $$ and $$ \\mathrm{cov}\\left(\\mathbf{f}_{*}\\right) =k\\left(X_{*}, X_{*}\\right)-k\\left(X_{*}, X\\right)\\left[k(X, X)+\\sigma^{2} I\\right]^{-1} k\\left(X, X_{*}\\right). $$ Exact inference in the GP model has computational complexity $\\mathcal{O}(m^3)$ due to the need to invert the $m \\times m$ matrix $k\\left(X_{*}, X\\right)+\\sigma^2I$, which restricts the application of exact GPs to small datasets. Approximations have been developed that reduce the complexity to $\\mathcal{O}(mp^2)$, where $p$ is some number of inducing points $p\u0026lt;m$ (Quionero-Candela and Rasmussen, 2005, Titsias, 2009). Along with the computational complexity, inference using GP models is challenging due to the need to select or design an appropriate kernel (with corresponding hyperparameters) for the task at hand.\nNeural processes The term neural processes was introduced to describe a family of models which use neural networks to model stochastic processes with complex joint distributions, of which the first was introduced just a few years ago (Garnelo et al., 2018a) with many variants developed since.\nLet us consider that for some system, there is an underlying stochastic process, $\\mathcal{P}$, from which we may sample a function $f$. Each time we sample a function, we are given access to $m$ noisy observations ${x_i, y_i}_{i=1}^m$. Given these observations, we wish to predict the values of the function at $n$ additional inputs ${x_i}_{i=m+1}^{m+n}$. This corresponds to finding the posterior predictive distribution\n\\begin{equation} p(y_{m+1:m+n}|x_{1:m+n}, y_{1:m}) = \\int p(y_{m+1:m+n}|f, x_{m+1:m+n})p(f|x_{1:m+n}, y_{1:m}) df, \\end{equation} which is obtained by marginalising out $f$. Generally, this integral is intractable, which motivates the use of variational inference, which I introduced in a previous post. A characterising assumption made by neural processes is that all information about $f$ is captured by some global latent variable $z$, such that \\begin{equation} p(y_{m+1:m+n}|f, x_{m+1:m+n}) = p_{\\theta}(y_{m+1:m+n}|x_{m+1:m+n}, z), \\end{equation} where $\\theta$ represents the learnable parameters of the likelihood (in this case, the model parameters). We assume conditional independence of the function values, given $z$, such that the joint distribution is given by \\begin{align} p(y_{m+1:m+n}, z|x_{1:m+n}, y_{1:m}) \u0026amp;= p(z|x_{1:m}, y_{1:m}) \\prod_{i=m+1}^{n+m} p_{\\theta}(y_i|x_i, z). \\end{align} This is related to the posterior predictive distribution $p(y_{m+1:m+n}|x_{1:m+n}, y_{1:m})$ by Bayes\u0026rsquo; theorem \\begin{align} p(y_{m+1:m+n}|x_{1:m+n}, y_{1:m}) \u0026amp;= \\frac{p(y_{m+1:m+n}, z|x_{1:m+n}, y_{1:m})}{p(z|x_{1:m+n}, y_{1:m+n})} \\\\\n\u0026amp;= \\frac{p(z|x_{1:m}, y_{1:m}) \\prod_{i=m+1}^{m+n} p_{\\theta}(y_i|x_i, z)}{p(z|x_{1:m+n}, y_{1:m+n})}. \\end{align} We introduce the variational distribution $q_{\\phi}(z|x_{1:m+n}, y_{1:m+n})$ which serves as an approximation to the posterior $p(z|x_{1:m+n}, y_{1:m+n})$. Following the framework laid out in a previous post on variational inference, we find that \\begin{align} \\small \\log{p(y_{m+1:m+n}|x_{1:m+n}, y_{1:m})} = \\mathcal{L}_{\\mathrm{ELBO}} + \\textrm{KL}[q_{\\phi}(z|x_{1:m+n}, y_{1:m+n}) || p(z|x_{1:m+n}, y_{1:m})] \\end{align} where \\begin{equation} \\small \\mathcal{L}_{\\mathrm{ELBO}} = \\mathbb{E}_{q_{\\phi}(z|x_{1:m+n}, y_{1:m+n})}\\left[\\log \\left(\\frac{p(z|x_{1:m}, y_{1:m})}{q_{\\phi}(z|x_{1:m+n}, y_{1:m+n})}\\right) + \\sum_{i=m+1}^{m+n} \\log p_{\\theta}(y_i|x_i, z) \\right] \\end{equation} In practice, the conditional prior $p(z|x_{1:m}, y_{1:m})$ is unknown, so we will approximate it with $q_{\\phi}(z|x_{1:m}, y_{1:m})$, such that \\begin{equation} \\small \\mathcal{L}_{\\mathrm{ELBO}} = \\mathbb{E}_{q_{\\phi}(z|x_{1:m+n}, y_{1:m+n})}\\left[\\log \\left(\\frac{q_{\\phi}(z|x_{1:m}, y_{1:m})}{q_{\\phi}(z|x_{1:m+n}, y_{1:m+n})}\\right) + \\sum_{i=m+1}^{m+n} \\log p_{\\theta}(y_i|x_i, z) \\right] \\end{equation} The relationship between the ELBO and the marginal likelihood enables us to find an approximation to the model parameters which maximise the marginal likelihood, and to the variational parameters which best approximate the posterior. This is achieved by jointly optimising the ELBO with respect to variational and model parameters. 3\nEmploying the amortised variational inference framework outlined here, $q_{\\phi}$ is parameterised by an encoder function $h$, which takes as input each observation tuple $(x_i, y_i)$ and outputs an encoding $r_i$, and a permutation invariant aggregation function $a$ which aggregates these encodings, usually using a simple mean or sum operation. This aggregated encoding, $r$, is the input to a neural network, which outputs the parameters of a Gaussian distribution over the latent variable $z$: \\begin{equation} z \\sim \\underbrace{\\mathcal{N}(z| \\mu_{\\phi}(r), \\sigma^2_{\\phi}(r))}_{q_{\\phi}(z|x_{1:m+n}, y_{1:m+n})}. \\end{equation} The variational parameters $\\phi$ are the parameters of these functions. In the Conditional Neural Process variant (Garnelo et al., 2018a), $\\mu_{\\phi}$ and $\\sigma_{\\phi}$ are defined as $\\mu_{\\phi}(r)=r; \\sigma^2_{\\phi}(r)=0$, however in general these functions are learned.\nFor real-valued outputs, the likelihood $p_{\\theta}(y_i|x_i, z)$ is modelled as \\begin{equation} p_{\\theta}(y_i|x_i, z) = \\mathcal{N}(z|\\mu_{\\theta}(x_i, z), \\sigma_{\\theta}^2(x_i, z)). \\end{equation} Here, ${\\mu_y, \\sigma_y}$ are the outputs of a decoder function with parameters $\\theta$ which takes as input the concatenation of $x_i$ and $z$. 4\nTo reiterate, the set of model parameters (which corresponds to the parameters of the decoder function) are learnt concurrently with the set of variational parameters (which corresponds to the parameters of the encoder function), through maximisation of $\\mathcal{L}_{ELBO}$. At test time, inference using the NP requires a single forward pass through the model, with an associated computational cost of $\\mathcal{O}(n+m)$. For large datasets, this represents a great reduction in computational cost compared to inference using an exact GP. It should be noted that this does not account for the time taken to train the NP which is often significant.\nMaking predictions The predictive distribution is given by \\begin{equation} p(y_{m+1}|x_{1:m+1}, y_{1:m}) = \\int p(y_{m+1:m+n}|x_{m+1}, z)p(z|x_{1:m}, y_{1:m}) dz. \\end{equation} We approximate this using the approximate posterior in place of the true posterior \\begin{equation} p(y_{m+1}|x_{1:m+1}, y_{1:m}) \\approx \\int p(y_{m+1}|x_{m+1}, z)q_{\\phi^*}(z|x_{1:m}, y_{1:m}) dz \\end{equation} This still amounts to propagating a Gaussian distribution through a non-linear transformation. Instead, we use a Monte Carlo estimate as \\begin{equation} p(y_{m+1}|x_{1:m+1}, y_{1:m}) \\approx \\frac{1}{T}\\sum_{t=1}^{T} p(y_{m+1}|x_{m+1}, z^{(t)}); \\quad z^{(t)} \\sim q_{\\phi^*}(z|x_{1:m}, y_{1:m}), \\end{equation} where $T$ is the number of samples.\nExample: 1D regression To demonstrate how training of these models occurs in practice and to study the relative performance of each model, I consider a one-dimensional regression task. Here, the data comprises a set of 1000 datasets, each of which contains between 5 and 100 tuples $(x, y)$ and is a randomly generated sample from a GP prior with mean and covariance functions given by\n\\begin{equation} \\small m(x) = 0 \\end{equation} \\begin{equation} \\small k(x, x\u0026rsquo;) = \\sigma_1^2\\left(1 + \\frac{\\sqrt{3}r}{l_1}\\right)\\exp{\\left(-\\frac{\\sqrt{3}r}{l_1}\\right)} + \\sigma_2^2\\left(1 + \\frac{\\sqrt{5}r}{l_2} + \\frac{5r^2}{3l_2^2}\\right)\\exp{\\left(-\\frac{\\sqrt{5}r}{l_2}\\right)}, \\end{equation} where $r=|x - x'|$, $\\sigma_1^2 = 2$, $l_1 = 1$, $\\sigma_2^2 = 1$ and $l_2 = 2$.\nI compare the performance of the Gaussian process with the squared exponential kernel (with learnable hyperparameters), the original Neural Process (NP), the Conditional Neural Process (CNP) and the Attentive Neural Process (ANP) (Garnelo et al., 2018a, Garnelo et al., 2018b, Kim et al., 2019). My Python implementation of these models can be found here.\n1D regression task predictions for two different datasets, the first (a-d) having $n \\gg m$ and the second (e-h) having $n \\ll m$. For both cases, from left to right, predictions are made by the GP (squared exponential kernel), CNP, NP, ANP respectively.\nWe see that the GP produces a good fit but could be considered overly smooth. The NP and CNP both suffer from underfitting which the ANP resolves by incorporating attention. Both the NP and the ANP are less smooth than the CNP which could be attributed to the need to take a finite number of samples of $z$ and $y$ to calculate the mean and variance of the output.\nNext time\u0026hellip;  In this post I gave an overview of how we can learn to model stochastic processes using probabilistic machine learning models, and specifically through learning the hyperparameters of a Gaussian process (by maximising the marginal likelihood), or through optimisation of the parameters of a neural process. This is one of a number of ways that we can incorporate uncertainty in a principled manner.\nIn my next post I plan to delve into another topic entirely, namely reinforcement learning.\nThanks for reading! Questions and feedback are always much appreciated - please don\u0026rsquo;t hesitate to get in touch.\nReferences (Karlin, 1968): Samuel Karlin. Elements of Stochastic Processes. In A First Course in Stochastic Processes, Elsevier, 1968.\n(Florescu, 2014) Ionut Florescu. Probability and Stochastic Processes. John Wiley \u0026amp; Sons, 2014.\n(Rasmussen and Williams, 2006) Carl E. Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning. MIT Press, 2006.\n(Snoek, Larochelle and Adams, 2012) Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian Optimization of Machine Learning Algorithms. NIPS, 2012.\n(Kuss and Rasmussen, 2004) Malte Kuss and Carl E. Rasmussen. Gaussian Processes in Reinforcement Learning. Advances in Neural Information Processing Systems, 2004.\n(Quionero-Candela and Rasmussen, 2005) Joaquin Quionero-Candela and Carl E. Rasmussen. A Unifying View of Sparse Approximate Gaussian Process Regression. Journal of Machine Learning Research, 2005.\n(Titsias, 2009) Michalis K. Titsias. Variational Learning of Inducing Variables in Sparse Gaussian Processes. Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS), 2009.\n(Garnelo et al., 2018a) Marta Garnelo, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo J. Rezende, and S. M. Ali Eslami. Conditional Neural Processes. Proceedings of the 35th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.\n(Garnelo et al., 2018b) Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S. M. Ali Eslami, and Yee Whye Teh. Neural Processes. ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative Models, 2018.\n(Kim et al., 2019) Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, and Yee Whye Teh. Attentive Neural Processes. ICLR, 2019.\n  Note that this is not the conventional notation for a stochastic process, which usually has the index parameter labelled $t$, for time, with corresponding random variable $X_t$. This is because of the usual interpretation of a stochastic process as describing the evolution of a randomly evolving system over time. Here we breach convention to generalise the index parameter to an arbitrary number of dimensions. \u0026#x21a9;\u0026#xfe0e;\n In general, we cannot study the joint distribution of an infinite number of random variables (Florescu, 2014). \u0026#x21a9;\u0026#xfe0e;\n A fully Bayesian approach would specify a prior distribution over model parameters and then compute the corresponding posterior, which is then integrated over when computing posterior predictive distributions. \u0026#x21a9;\u0026#xfe0e;\n The decoder is trying to produce $f(x_i)$. To do so, it needs to have knowledge both of $f$, and the input location $x_i$. Assuming that $z$ is an adequate summary of $f$, we anticipate that for a flexible enough likelihood, a forward pass through the decoder will be able to reproduce $f(x_i)$ from $z$ and $x_i$. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1605484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605484800,"objectID":"f9119de922e7b1afeb3f9c3f97df48d5","permalink":"https://penelopejones.github.io/post/neural_processes/","publishdate":"2020-11-16T00:00:00Z","relpermalink":"/post/neural_processes/","section":"post","summary":"ML can be used to learn to model distributions over functions.","tags":["Bayesian inference","Machine learning","Uncertainty quantification","Stochastic processes"],"title":"ML and Stochastic Processes","type":"post"},{"authors":["Penelope Jones"],"categories":[],"content":"In my last post, I introduced Bayesian inference as a principled framework for quantifying our level of (un)certainty, in models and predictions. From that post, it can be observed that almost all computations of the quantities of interest (such as the marginal likelihood) in Bayesian inference demand the integration of complex functions over all parameters of the model of interest. In practice, these integrals are usually intractable.\nMuch work in Bayesian machine learning is focused on developing approximate inference methods that circumvent the challenge of computing exact integrals. These techniques are generally demarcated as Markov Chain Monte Carlo (MCMC) and variational inference methods.\nMCMC methods are\n computationally expensive, scale badly with large amounts of data, and it is difficult to assess their convergence, but they are mathematically guaranteed to converge exactly in the limit of infinite data.  In contrast, variational inference methods are\n computationally cheaper, scale well with large quantities of data, but typically do not converge to the true distribution in the infinite data limit.  Variational inference Variational inference (Blei et al., 2017) methods introduce a variational distribution $q_{\\phi}(\\theta)$ that upon optimisation of the variational parameters $\\phi$ will be used as an approximation to some true distribution $p(\\theta)$. The power of this family of techniques lies in their ability to simultaneously approximate the posterior distribution over model parameters and also the marginal likelihood of the model.\nThe general approach to VI is as follows. Suppose that we seek to determine both the marginal likelihood $p(X|M)$ of some model $M$, and also to determine the posterior over model parameters $\\theta$ given the observed data, $p(\\theta|X, M)$. These two quantities are related by Bayes\u0026rsquo; theorem $$p(\\theta|X, M) = \\frac{p(X, \\theta|M)}{p(X|M)}.$$ We now introduce the variational distribution $q_{\\phi}(\\theta)$, which will be used as an approximation to $p(\\theta|X, M)$. Rearranging, and inserting the factor $1 = \\frac{q_{\\phi}(\\theta)}{q_{\\phi}(\\theta)}$ gives $$p(X|M) = \\frac{p(X, \\theta|M)}{q_{\\phi}(\\theta)}\\frac{q_{\\phi}(\\theta)}{p(\\theta|X, M)}.$$ Taking the logarithm of both sides, multiplying by $q_{\\phi}(\\theta)$ and integrating over $\\theta$, we obtain the central relation $$\\log{p(X|M)} = \\mathcal{L}_\\mathrm{ELBO} + \\mathrm{KL}[q_{\\phi}(\\theta)||p(\\theta|X, M)]$$ where the first term is denoted the evidence lower bound (ELBO) $$\\begin{aligned} \\mathcal{L}_\\mathrm{ELBO} \u0026amp;= \\mathbb{E}_{q_{\\phi}(\\theta)}\\left[\\log{\\frac{p(X, \\theta|M)}{q_{\\phi}(\\theta)}}\\right] \\\\\n\u0026amp;= \\int q_{\\phi}(\\theta) \\log{\\frac{p(X, \\theta|M)}{q_{\\phi}(\\theta)}} d\\theta,\\end{aligned}$$ and the second term \\begin{equation} \\mathrm{KL}[q_{\\phi}(\\theta)||p(\\theta|X, M)] = \\int q_{\\phi}(\\theta) \\log{\\frac{q_{\\phi}(\\theta)}{p(\\theta|X, M)}} d\\theta \\end{equation} is the KullbackLeibler (KL) divergence between $q_{\\phi}(\\theta)$ and $p(\\theta|X, M)$ (Kullback and Leibler, 1951). Crucially, the KL divergence between two distributions is always non-negative, with equality iff $q_{\\phi}(\\theta) \\equiv p(\\theta|X, M)$ (Hershey and Olsen, 2007). By minimising the KL divergence (or equivalently, by maximising $\\mathcal{L}_\\mathrm{ELBO}$) with respect to variational parameters $\\phi$, we achieve two goals. Firstly, we can use the optimal variational parameters $\\phi^*$ to form our approximation to $p(\\theta|X, M)$, using $q^*(\\theta) = q_{\\phi^*}(\\theta)$. Secondly, we will obtain a lower bound and approximation to the marginal likelihood via the maximised $\\mathcal{L}_\\mathrm{ELBO}$. We see that VI turns an intractable integration problem into a tractable optimisation problem which can be solved using techniques such as stochastic gradient descent.\nAmortised variational inference In contrast to mean field variational inference for which each datapoint has an associated set of variational parameters, in amortised variational inference the variational parameters are shared across all data points (Gershman and Goodman, 2014). Typically this is achieved using a neural network to learn a mapping from observations to latent variables. The neural network parameters are the variational parameters to be optimised. The benefit of this is that the number of variational parameters remains fixed with respect to the number of data points. Also, inference can be performed on unseen data at the cost of a single forward pass through the so-called `inference network\u0026rsquo;.\nNext time\u0026hellip; In this post, I have provided an overview of variational inference. 1 In my next post I will demonstrate how I have used variational inference to distil physical insights from soft matter systems.\nThanks for reading! Feedback always appreciated.\nReferences (Blei et al., 2017) David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statisticians. Journal of the American Statistical Association, 112(518):859877, April 2017.\n(Kullback and Leibler, 1951) Solomon Kullback and Richard .A. Leibler. On Information and Sufficiency. Annals of Mathematical Statistics, 22(1):7986, March 1951.\n(Hershey and Olsen, 2007) John R. Hershey and Peder A. Olsen. Approximating the Kullback-Leibler Divergence Between Gaussian Mixture Models. In 2007 IEEE International Conference on Acoustics, Speech and Signal Processing 320, April 2007. ISSN: 2379-190X.\n(Gershman and Goodman, 2014) Samuel J. Gershman and Noah D. Goodman. Amortized Inference in Probabilistic Reasoning.\n  I plan to cover MCMC methods in a future post! \u0026#x21a9;\u0026#xfe0e;\n   ","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603065600,"objectID":"0eeb24a2894a0ab7577fb88224cfca14","permalink":"https://penelopejones.github.io/post/variational_inference/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/post/variational_inference/","section":"post","summary":"A beautiful method for approximating intractable integrals.","tags":["Bayesian inference","Approximate inference","Optimisation","Machine learning"],"title":"An Introduction to Variational Inference","type":"post"},{"authors":["Penelope Jones"],"categories":[],"content":"The rise of big data has been ubiquitous in almost all scientific fields as well as in broader society; the five most valuable companies in the United States are fundamentally data-driven. 1 It is hardly surprising then, that media outlets and industrial experts alike have proclaimed that data is the new oil in reference to the mass commoditisation of data globally (Humby, 2015, The Economist, 2017, Yi et al., 2014).\nThe real value in big data has arisen not only from the data itself, but from the concinnity of this data collection with ever-improving computing power and significant advancements in the tools available for making sense of the data. Many of these tools have been branded by the umbrella term machine learning. At the core of these methodologies is the inference of useful insights from our observations, which can be used to make predictions about unobserved variables, to build models of complex systems and to guide decision-making processes. In science, machine learning is routinely used in a plethora of applications, from designing novel molecules for use as medicinal drugs (Chodera et al., 2020), to aiding conservation of coral reefs (Nunes et al., 2020), and, topically, for forecasting the spread of infectious diseases such as that of the novel coronavirus COVID-19 (Chae et al., 2018, Bertozzi et al., 2020).\nA burgeoning focus of research within machine learning is on the incorporation of probabilistic methodologies to enable the quantification of uncertainty. This is desirable in many applications of machine learning in science where it is vital that we know what we do not know. At the heart of most probabilistic models is the Bayesian framework which, as we shall see, provides a principled mechanism by which ones degree of belief in any event or model can be iteratively updated, and which enables the quantification of uncertainty in every prediction that one makes.\nThe omnipresence of uncertainty in science A scientist recognises and appreciates the success of prevailing theories and models in explaining her observations, but is all the while cognisant that no theory should be above scrutiny. The course of history is testament to this world view; countless theories once considered indubitable truths have been revealed to be flawed. A corollary of this is that no model should ever be considered certain. Instead, one should assess the extent to which they believe that a model is true based on the information available to them.\nIt is on this premise that the Bayesian view of probability naturally arises. Here, anything about which one is uncertain is assigned a probability between zero and one, which quantifies one\u0026rsquo;s degree of belief that the unobserved quantity will take a specific value. A value close to zero implies a belief that it is unlikely an event will occur. Conversely a probability close to one implies a belief that an event is likely to occur.\nNo aspect of the scientific method is free of uncertainty. When we make a hypothesis using a model, we consciously or subconsciously assign a prior belief to that hypothesis based on information acquired from previous observations. When this belief is sufficiently great to warrant our time and efforts, we may test the hypothesis by conducting an experiment. Inevitably, even our experimental observations are uncertain - our measurements are subject to noise in part due to the finite precision of our measuring devices. 2 Finally, we use these observations to update our degree of belief in the model, based on its ability to explain the data.\nThe Bayesian approach to inference The Bayesian approach to inference provides a principled framework for the quantification of uncertainty at every level. Using a Bayesian framework, one can iteratively update their beliefs with every new observation. This continual cycle of making observations and then updating our beliefs based on those observations is conceptually satisfying, as it is in accordance with our own life experiences.\nThe Bayesian paradigm is to update our degree of belief in a model $M$ in light of new data $X$ using Bayes theorem, $$p(M|X) = \\frac{p(X|M)p(M)}{p(X)}.$$ Here $p(M)$ is the prior, which quantifies our degree of belief in $M$ before observing $X$, and $p(M|X)$ is the posterior, which quantifies our degree of belief in $M$ after observing $X$. The posterior is related to the prior through the likelihood $p(X|M)$, which measures how well $M$ can explain $X$, and the marginal likelihood $p(X)$, which here serves as a normalising constant. When new observations are made, the previous posterior serves as the new prior, embodying the iterative nature of learning.\nThere are multiple modelling questions that we may wish to answer, all of which involve performing inference at different levels of detail. For example, we may want to determine whether data is best explained by a polynomial function, or by a neural network. Alternatively, we may be happy to assume a polynomial function can explain the data, but seek to infer the most probable order of that polynomial function. Finally, at the lowest level, we may be happy to fix the order of the polynomial, but wish to infer its coefficients. We can answer all of these questions using Bayesian inference.\nLet $\\mathcal{M}$ denote a family of models. Each member of the family, $M\\in\\mathcal{M}$, is defined by a set of hyperparameters and consists of model parameters $_M$. Table 1 provides some common examples of such models and their associated hyperparameters and parameters.\nTable 1: Examples of models and their hyperparameters and parameters.\n   Model family $\\mathcal{M}$ Defining hyperparameters of model $M\\in \\mathcal{M}$ Model parameters $\\theta_M$     Polynomial function The order, $n$, of the polynomial Coefficients $C_n={c_i}_{i=0}^n$   Gaussian mixture model Number of components Means, variances, mixing coefficients   Neural network Number of layers, size of layers, choice of activation function Neural network parameters    The probability of observing a particular dataset is found by integrating out all other variables from the joint distribution $$\\begin{aligned} p(X) =\u0026amp; \\sum_{\\mathcal{M}} \\sum_{M} \\int p(X, \\theta_M, M, \\mathcal{M}) d\\theta_M \\nonumber \\\\\n=\u0026amp; \\sum_{\\mathcal{M}} p(\\mathcal{M}) \\sum_{M}p(M|\\mathcal{M})\\int p(\\theta_M|M, \\mathcal{M}) p(X|\\theta_M, M, \\mathcal{M}) d\\theta_M.\\end{aligned}$$\nThis equation is rather daunting. It is more intuitive to separate the procedure into three levels. The level(s) of inference that shall be considered will depend on what the statistician hopes to achieve, and the extent to which she is willing to restrict the breadth of models considered.\nAt the first level of inference, the model is specified and fixed. The goal is to determine the posterior $p(\\theta_M|X, M, \\mathcal{M})$ over the models parameters $\\theta_M$, given some observed data $X$. We must specify the prior $p(\\theta_M|M, \\mathcal{M})$ and the likelihood $p(X|\\theta_M, M, \\mathcal{M})$. The posterior is $$p(\\theta_M|X, M, \\mathcal{M}) = \\frac{p(X|\\theta_M, M, \\mathcal{M})p(\\theta_M|M, \\mathcal{M})}{p(X|M, \\mathcal{M})}.$$ The normalising constant $p(X|M, \\mathcal{M})$ is the marginal likelihood of the model, and is computed by integrating over $\\theta_M$ $$p(X|M, \\mathcal{M}) = \\int p(\\theta_M| M, \\mathcal{M}) p(X|\\theta_M, M, \\mathcal{M}) d\\theta_M.$$\nThis quantity is central to the second level of inference, in which the model family remains fixed, but now we seek to compare the relative plausibility of two models in the same model family. We can compute the posterior probability of model $M$ as $$ p(M|X, \\mathcal{M}) = \\frac{p(X|M, \\mathcal{M})p(M| \\mathcal{M})}{p(X|\\mathcal{M})},$$\nwhere $p(X|\\mathcal{M})$ is the marginal likelihood of the model family: $$p(X|\\mathcal{M}) = \\sum_{M \\in \\mathcal{M}} P(X|M, \\mathcal{M})p(M|\\mathcal{M}).$$\nAssuming a uniform prior over models such that $p(M|\\mathcal{M}) \\propto 1$, the model that best explains the data will be that which maximises $p(X|M, \\mathcal{M})$. The marginal likelihood penalises overly complex models and has an implicit Occams razor property: the optimal model will be the simplest model that can explain the data adequately (MacKay, 1991).\nAt the highest level of inference, we could compare the ability of two different model families to explain $X$. This can be achieved by computing the posterior distribution of the model family $$p(\\mathcal{M}|X) = \\frac{p(X|\\mathcal{M})p(\\mathcal{M})}{p(X)}.$$ The normalising constant $p(X)$ is independent of $\\mathcal{M}$, and typically the prior over model families $p(\\mathcal{M})$ is assumed uniform. 3\nPredicting the values of missing data Given the observed dataset $X$, we may wish to predict the values of some unobserved data, $X^{\\star}$. In parametric models, $X^{\\star}$ is assumed conditionally independent of $X$ given the model parameters, such that the distribution $p(X^{\\star}|X, M)$ is $$p(X^{\\star}|X, M) = \\int p(X^{\\star}|\\theta_M, M) p(\\theta_M|X, M) d\\theta_M.$$\nNext time\u0026hellip; In this post, I outlined the importance of uncertainty quantification in science, and showed how Bayesian inference can be used to update our degree of belief in measurements and models in light of new observations.\nNext time, I will be covering the basics of variational inference, an important family of techniques for approximating the intractable integrals that are ubiquitous in Bayesian inference.\nThanks for reading!\nReferences (Forbes, 2020) Global 2000 - The Worlds Largest Public Companies 2020, August 2020.\n(Humby, 2015) Clive Humby. Data is the new oil, 2015.\n(The Economist, 2017) The worlds most valuable resource is no longer oil, but data. The Economist, May 2017.\n(Yi et al., 2014) Xiaomeng Yi, Fangming Liu, Jiangchuan Liu, and Hai Jin. Building a network highway for big data: architecture and challenges. IEEE Network, 28(4):513, July 2014. Conference Name: IEEE Network.\n(Chodera et al., 2020) John Chodera, Alpha A. Lee, Nir London, and Frank von Delft. Crowdsourcing drug discovery for pandemics. Nature Chemistry, 12(7):581581, July 2020. Number: 7 Publisher: Nature Publishing Group.\n(Nunes et al., 2020) Jose Anchieta C. C. Nunes, Igor C. S. Cruz, Andre Nunes, and Hudson T. Pinheiro. Speeding up coral reef conservation with AI-aided automated image analysis. Nature Machine Intelligence, 2(6):292292, June 2020. Number: 6 Publisher: Nature Publishing Group.\n(Chae et al., 2018) Sangwon Chae, Sungjun Kwon, and Donghyun Lee. Predicting Infectious Disease Using Deep Learning and Big Data. International Journal of Environmental Research and Public Health, 15(8), August 2018.\n(Bertozzi et al., 2020) Andrea L. Bertozzi, Elisa Franco, George Mohler, Martin B. Short, and Daniel Sledge. The challenges of modeling and forecasting the spread of COVID-19. Proceedings of the National Academy of Sciences, 117(29):1673216738, July 2020. Publisher: National Academy of Sciences Section: Physical Sciences.\n(MacKay, 1991) David J.C. MacKay. Bayesian Methods for Adaptive Models. PhD Thesis, December 1991.\n  As of August 2020, the five most valuable publicly traded US companies by market capitalisation are Alphabet, Amazon, Apple, Facebook and Microsoft (Forbes, 2020). \u0026#x21a9;\u0026#xfe0e;\n The uncertainty associated with observation noise is termed aleatoric, as against the epistemic uncertainty that arises due to a lack of observations. \u0026#x21a9;\u0026#xfe0e;\n This is because there is usually no reason for us to assign a prior degree of belief to a particular model family. Thus we assume p()1. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1602460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602460800,"objectID":"0592f883916e8e05d404ffe556f9680f","permalink":"https://penelopejones.github.io/post/bayesian/","publishdate":"2020-10-12T00:00:00Z","relpermalink":"/post/bayesian/","section":"post","summary":"Uncertainty is everywhere... Bayesian inference can help us to \"know what we do not know\".","tags":["Bayesian inference","Machine learning","Uncertainty quantification"],"title":"Bayesian Inference","type":"post"},{"authors":["Penelope Jones"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://penelopejones.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Welcome to Slides academia\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://penelopejones.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using academia's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"a903a8fd061907c9182a22fe4aed4729","permalink":"https://penelopejones.github.io/project/artificial-intelligence/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/artificial-intelligence/","section":"project","summary":"Lorem ipsum dolor sit amet consectetur adipisicing elit. Magnam, eius.","tags":null,"title":"Artificial Intelligence","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"cbe0c24c174e9c23efd27e59e7e56acc","permalink":"https://penelopejones.github.io/project/deep-learning/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/deep-learning/","section":"project","summary":"Lorem ipsum dolor sit amet consectetur adipisicing elit. Magnam, eius.","tags":null,"title":"Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://penelopejones.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using \"external_link\".","tags":null,"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://penelopejones.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":null,"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2deecdf5e31bc09985d9db8bcee07248","permalink":"https://penelopejones.github.io/project/robotics/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/robotics/","section":"project","summary":"Lorem ipsum dolor sit amet consectetur adipisicing elit. Magnam, eius.","tags":null,"title":"Robotics","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e3d2cb9f760d662f43826965ea291872","permalink":"https://penelopejones.github.io/project/skin-cancer-ai/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/skin-cancer-ai/","section":"project","summary":"Lorem ipsum dolor sit amet consectetur adipisicing elit. Magnam, eius.","tags":null,"title":"Skin Cancer AI","type":"project"},{"authors":["Penelope Jones"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://penelopejones.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"An example journal article","type":"publication"},{"authors":["Penelope Jones"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://penelopejones.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"An example conference paper","type":"publication"}]