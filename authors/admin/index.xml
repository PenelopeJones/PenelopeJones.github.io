<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Penelope Jones</title>
    <link>https://penelopejones.github.io/authors/admin/</link>
    <description>Recent content on Penelope Jones</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Mon, 16 Nov 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://penelopejones.github.io/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ML and Stochastic Processes</title>
      <link>https://penelopejones.github.io/post/neural_processes/</link>
      <pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://penelopejones.github.io/post/neural_processes/</guid>
      <description>&lt;p&gt;An assortment of physical systems exist that can be considered stochastic
processes. For example, diffusion and heat transfer can be modelled as
Brownian processes, and the number of decays from a radioactive sample
can be modelled as a Poisson process. In daily life, social network dynamics, traffic flow
and stock market prices are routinely modelled using the mathematics of stochastic processes [1].&lt;/p&gt;
&lt;p&gt;A stochastic process, $\mathcal{P}$, defines a family of random variables,
which may have a finite or infinite number of components. Each realisation
of a stochastic process ${y_x, x \in \mathcal{X}}$ is an assignment of a possible
value of $y_x$ to each $x \in \mathcal{X}$ (&lt;a href=&#34;https://www.sciencedirect.com/book/9781483230993/a-first-course-in-stochastic-processes&#34;&gt;Karlin, 1968&lt;/a&gt;).
When the domain $\mathcal{X}$ is real-valued,
such a realisation corresponds to a function, $f$, mapping $x$ to a corresponding value
$y_x$. &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Thus we can interpret a stochastic process as defining a distribution over functions,
with each sample from $\mathcal{P}$ corresponding to a different function.&lt;/p&gt;
&lt;p&gt;We can completely characterise a stochastic process by specifying the joint distribution of every
finite family $y_{x_1}, y_{x_2}, &amp;hellip; , y_{x_n}$ of variables. &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; By placing restrictions
on the form of the joint distribution, inference can be made tractable. One such example of this is
the Gaussian process, a widely used stochastic process that has found success in
a variety of domains.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-processes&#34;&gt;Gaussian processes&lt;/h2&gt;
&lt;p&gt;A Gaussian process (GP) is a stochastic process for which the joint distribution
of any finite subset of the set of random variables is Gaussian.
GPs are used extensively in a range of applications, including in Bayesian
optimisation, reinforcement learning and property prediction.&lt;/p&gt;
&lt;p&gt;A GP is wholly specified by a mean function $m(x)$ and covariance
function $k(x, x&amp;rsquo;)$, where for $\mathit{f} \sim \mathcal{P}$:
$$\begin{aligned}
m(x) &amp;amp;= \mathbb{E}[f(x)] \\&lt;br&gt;
k(x, x&amp;rsquo;) &amp;amp;= \mathbb{E}[(f(x) - m(x))(f(x&amp;rsquo;) - m(x&amp;rsquo;))].\end{aligned}$$
Often $m(x)$ is set to be 0. The covariance function specifies the
covariance between the random variables $f(x)$ and $f(x&amp;rsquo;)$. For
commonly used examples such as the squared exponential and Matérn
covariance functions, the covariance is close to unity when inputs
$x$ and $x&#39;$ are in close proximity, but reduces to zero as
their separation distance increases.&lt;/p&gt;
&lt;p&gt;The choice of mean and covariance function dictates the properties of
the sampled functions, such as smoothness, lengthscale and periodicity.
Through these choices, the inductive biases are specified. For example,
consider the squared exponential kernel:
$$k(x, x&amp;rsquo;) = \exp{\left(-\frac{|x - x&#39;|^2}{2l^2} \right)}.$$&lt;/p&gt;
&lt;p&gt;Here, the hyperparameter $l$ is a measure of the lengthscale in input space over which
random variables are no longer significantly correlated. In practice, the prior hyperparameters
are learnt through maximisation of the marginal likelihood $p(X|l)$ (&lt;a href=&#34;http://www.gaussianprocess.org/gpml/&#34;&gt;Rasmussen and Williams, 2006&lt;/a&gt;),
as outlined in my &lt;a href=&#34;../bayesian&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let us now consider that we have made some noisy observations, $y= { y_i } _{i=1}^m$ at locations
$X={x_i}_{i=1}^m$.
We will assume that these observations are our window for gaining insights about the underlying function
$f$ from which the observations were made. We want to predict the value of the function at an
additional number of locations, $X^{*} = {x_i}_{i=m+1}^n$.&lt;/p&gt;
&lt;p&gt;Assuming that observations are corrupted by Gaussian noise with variance $\sigma^2$,
the joint distribution of the observed values $y$ and the test values $f_{*}$ is
$$
\left[\begin{matrix}
\mathbf{y} \\
\mathbf{f}_{*}
\end{matrix}\right] \sim \mathcal{N}\left(\mathbf{0},\left[\begin{matrix}
k(X, X)+\sigma^{2} I &amp;amp; k(X, X_{*}) \\&lt;br&gt;
k\left(X_{*}, X\right) &amp;amp; k\left(X_{*}, X_{*}\right)
\end{matrix}\right]\right).$$&lt;/p&gt;
&lt;p&gt;Conditioning on the observed values, we obtain the posterior distribution over $f_{*}$:&lt;/p&gt;
&lt;p&gt;$$
f_{*} | X, y, X_{*} \sim \mathcal{N}(\bar{f_{*}}, \mathrm{cov} \left(f_{*}\right))
$$&lt;/p&gt;
&lt;p&gt;where
$$
\bar{f_{*}} = k\left(X_{*}, X\right)\left[k\left(X, X\right)+\sigma^2I \right]^{-1} y
$$
and
$$
\mathrm{cov}\left(\mathbf{f}_{*}\right) =k\left(X_{*}, X_{*}\right)-k\left(X_{*}, X\right)\left[k(X, X)+\sigma^{2} I\right]^{-1} k\left(X, X_{*}\right).
$$
Exact inference in the GP model has computational complexity $\mathcal{O}(m^3)$ due to the need
to invert the $m \times m$ matrix $k\left(X_{*}, X\right)+\sigma^2I$, which restricts the
application of exact GPs to small datasets. Approximations have been developed that reduce the
complexity to $\mathcal{O}(mp^2)$, where $p$ is some number of inducing points $p&amp;lt;m$ (&lt;a href=&#34;https://www.jmlr.org/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf&#34;&gt;Quiñonero-Candela and Rasmussen, 2005&lt;/a&gt;,
&lt;a href=&#34;http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf&#34;&gt;Titsias, 2009&lt;/a&gt;).
Along with the computational complexity, inference using GP models is challenging due to the
need to select or design an appropriate kernel (with corresponding hyperparameters) for the
task at hand.&lt;/p&gt;
&lt;h2 id=&#34;neural-processes&#34;&gt;Neural processes&lt;/h2&gt;
&lt;p&gt;The term &lt;em&gt;neural processes&lt;/em&gt; was introduced to describe a family of models which use neural networks
to model stochastic processes with complex joint distributions, of which the first was introduced
just a few years ago (&lt;a href=&#34;https://arxiv.org/pdf/1807.01613.pdf&#34;&gt;Garnelo et al., 2018a&lt;/a&gt;) with many variants developed
since.&lt;/p&gt;
&lt;p&gt;Let us consider that for some system, there is an underlying stochastic process, $\mathcal{P}$,
from which we may sample a function $f$. Each time we sample a function, we are given access
to $m$ noisy observations ${x_i, y_i}_{i=1}^m$. Given these observations, we wish to
predict the values of the function at $n$ additional inputs ${x_i}_{i=m+1}^{m+n}$.
This corresponds to finding the posterior predictive distribution&lt;br&gt;
\begin{equation}
p(y_{m+1:m+n}|x_{1:m+n}, y_{1:m}) = \int p(y_{m+1:m+n}|f, x_{m+1:m+n})p(f|x_{1:m+n}, y_{1:m}) df,
\end{equation}
which is obtained by marginalising out $f$. Generally, this integral is intractable,
which motivates the use of variational inference, which I introduced in a &lt;a href=&#34;../variational_inference&#34;&gt;previous post&lt;/a&gt;.
A characterising assumption made by neural processes is that all information about $f$ is captured by some
global latent variable $z$, such that
\begin{equation}
p(y_{m+1:m+n}|f, x_{m+1:m+n}) = p_{\theta}(y_{m+1:m+n}|x_{m+1:m+n}, z),
\end{equation}
where $\theta$ represents the learnable parameters of the likelihood (in this case, the model parameters).
We assume conditional independence of the function values, given $z$, such that the joint distribution is given by
\begin{align}
p(y_{m+1:m+n}, z|x_{1:m+n}, y_{1:m}) &amp;amp;= p(z|x_{1:m}, y_{1:m}) \prod_{i=m+1}^{n+m} p_{\theta}(y_i|x_i, z).
\end{align}
This is related to the posterior predictive distribution $p(y_{m+1:m+n}|x_{1:m+n}, y_{1:m})$ by Bayes&amp;rsquo; theorem
\begin{align}
p(y_{m+1:m+n}|x_{1:m+n}, y_{1:m}) &amp;amp;= \frac{p(y_{m+1:m+n}, z|x_{1:m+n}, y_{1:m})}{p(z|x_{1:m+n}, y_{1:m+n})} \\&lt;br&gt;
&amp;amp;= \frac{p(z|x_{1:m}, y_{1:m}) \prod_{i=m+1}^{m+n} p_{\theta}(y_i|x_i, z)}{p(z|x_{1:m+n}, y_{1:m+n})}.
\end{align}
We introduce the variational distribution $q_{\phi}(z|x_{1:m+n}, y_{1:m+n})$ which serves as an
approximation to the posterior $p(z|x_{1:m+n}, y_{1:m+n})$. Following the framework laid out in a &lt;a href=&#34;../variational_inference&#34;&gt;previous post on
variational inference&lt;/a&gt;, we find that
\begin{align}
\small \log{p(y_{m+1:m+n}|x_{1:m+n}, y_{1:m})} = \mathcal{L}_{\mathrm{ELBO}} + \textrm{KL}[q_{\phi}(z|x_{1:m+n}, y_{1:m+n}) || p(z|x_{1:m+n}, y_{1:m})]
\end{align}
where
\begin{equation}
\small \mathcal{L}_{\mathrm{ELBO}} = \mathbb{E}_{q_{\phi}(z|x_{1:m+n}, y_{1:m+n})}\left[\log \left(\frac{p(z|x_{1:m}, y_{1:m})}{q_{\phi}(z|x_{1:m+n}, y_{1:m+n})}\right) + \sum_{i=m+1}^{m+n} \log p_{\theta}(y_i|x_i, z) \right]
\end{equation}
In practice, the conditional prior $p(z|x_{1:m}, y_{1:m})$ is unknown, so we will approximate it with $q_{\phi}(z|x_{1:m}, y_{1:m})$, such that
\begin{equation}
\small \mathcal{L}_{\mathrm{ELBO}} = \mathbb{E}_{q_{\phi}(z|x_{1:m+n}, y_{1:m+n})}\left[\log \left(\frac{q_{\phi}(z|x_{1:m}, y_{1:m})}{q_{\phi}(z|x_{1:m+n}, y_{1:m+n})}\right) + \sum_{i=m+1}^{m+n} \log p_{\theta}(y_i|x_i, z) \right]
\end{equation}
The relationship between the ELBO and the marginal likelihood enables us to find an approximation to the model parameters which maximise the marginal likelihood, and to the variational parameters which best approximate the posterior.
This is achieved by jointly optimising the ELBO with respect to variational and model parameters.
&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Employing the amortised variational inference framework outlined &lt;a href=&#34;../variational_inference&#34;&gt;here&lt;/a&gt;, $q_{\phi}$ is parameterised by an encoder function $h$,
which takes as input each observation tuple $(x_i, y_i)$ and outputs an encoding $r_i$, and a permutation invariant aggregation
function $a$ which aggregates these encodings, usually using a simple mean or sum operation.
This aggregated encoding, $r$, is the input to a neural network, which outputs the parameters
of a Gaussian distribution over the latent variable $z$:
\begin{equation}
z \sim \underbrace{\mathcal{N}(z| \mu_{\phi}(r), \sigma^2_{\phi}(r))}_{q_{\phi}(z|x_{1:m+n}, y_{1:m+n})}.
\end{equation}
The variational parameters $\phi$ are the parameters of these functions. In the Conditional Neural Process variant (&lt;a href=&#34;https://arxiv.org/pdf/1807.01613.pdf&#34;&gt;Garnelo et al., 2018a&lt;/a&gt;),
$\mu_{\phi}$ and $\sigma_{\phi}$ are defined as $\mu_{\phi}(r)=r; \sigma^2_{\phi}(r)=0$, however in general these functions are learned.&lt;/p&gt;
&lt;p&gt;For real-valued outputs, the likelihood $p_{\theta}(y_i|x_i, z)$ is modelled as
\begin{equation}
p_{\theta}(y_i|x_i, z) = \mathcal{N}(z|\mu_{\theta}(x_i, z), \sigma_{\theta}^2(x_i, z)).
\end{equation}
Here, ${\mu_y, \sigma_y}$ are the outputs of a decoder function with parameters $\theta$ which takes as input the
concatenation of $x_i$ and $z$. &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;To reiterate, the set of model parameters (which corresponds to the parameters of the decoder function) are learnt concurrently with the set of variational parameters (which corresponds to the parameters of the encoder function), through maximisation of $\mathcal{L}_{ELBO}$. At test time, inference using the NP requires a single forward pass through the model, with an associated computational cost of $\mathcal{O}(n+m)$. For large datasets, this represents a great reduction in computational cost compared to inference using an exact GP. It should be noted that this does not account for the time taken to train the NP which is often significant.&lt;/p&gt;
&lt;h3 id=&#34;making-predictions&#34;&gt;Making predictions&lt;/h3&gt;
&lt;p&gt;The predictive distribution is given by
\begin{equation}
p(y_{m+1}|x_{1:m+1}, y_{1:m}) = \int p(y_{m+1:m+n}|x_{m+1}, z)p(z|x_{1:m}, y_{1:m}) dz.
\end{equation}
We approximate this using the approximate posterior in place of the true posterior
\begin{equation}
p(y_{m+1}|x_{1:m+1}, y_{1:m}) \approx \int p(y_{m+1}|x_{m+1}, z)q_{\phi^*}(z|x_{1:m}, y_{1:m}) dz
\end{equation}
This still amounts to propagating a Gaussian distribution through a non-linear transformation. Instead, we use a Monte Carlo estimate as
\begin{equation}
p(y_{m+1}|x_{1:m+1}, y_{1:m}) \approx \frac{1}{T}\sum_{t=1}^{T} p(y_{m+1}|x_{m+1}, z^{(t)}); \quad z^{(t)} \sim q_{\phi^*}(z|x_{1:m}, y_{1:m}),
\end{equation}
where $T$ is the number of samples.&lt;/p&gt;
&lt;h2 id=&#34;example-1d-regression&#34;&gt;Example: 1D regression&lt;/h2&gt;
&lt;p&gt;To demonstrate how training of these models occurs in practice and to study the
relative performance of each model, I consider a one-dimensional regression task.
Here, the data comprises a set of 1000 datasets, each of which contains between 5 and
100 tuples $(x, y)$ and is a randomly generated sample from a GP prior with mean and
covariance functions given by&lt;/p&gt;
&lt;p&gt;\begin{equation}
\small m(x) = 0
\end{equation}
\begin{equation}
\small k(x, x&amp;rsquo;) = \sigma_1^2\left(1 + \frac{\sqrt{3}r}{l_1}\right)\exp{\left(-\frac{\sqrt{3}r}{l_1}\right)} + \sigma_2^2\left(1 + \frac{\sqrt{5}r}{l_2} + \frac{5r^2}{3l_2^2}\right)\exp{\left(-\frac{\sqrt{5}r}{l_2}\right)},
\end{equation}
where $r=|x - x&#39;|$, $\sigma_1^2 = 2$, $l_1 = 1$, $\sigma_2^2 = 1$ and $l_2 = 2$.&lt;/p&gt;
&lt;p&gt;I compare the performance of the Gaussian process with the squared exponential kernel
(with learnable hyperparameters), the original Neural Process (NP), the Conditional Neural Process (CNP)
and the Attentive Neural Process (ANP) (&lt;a href=&#34;https://arxiv.org/pdf/1807.01613.pdf&#34;&gt;Garnelo et al., 2018a&lt;/a&gt;,
&lt;a href=&#34;https://arxiv.org/abs/1807.01622&#34;&gt;Garnelo et al., 2018b&lt;/a&gt;,
&lt;a href=&#34;https://arxiv.org/abs/1901.05761&#34;&gt;Kim et al., 2019&lt;/a&gt;). My Python implementation of these models can be found
&lt;a href=&#34;https://github.com/PenelopeJones/neural_processes&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;predictions.png&#34; alt=&#34;&#34;&gt;
&lt;em&gt;1D regression task predictions for two different datasets,
the first (a-d) having $n \gg m$ and the second
(e-h) having $n \ll m$. For both cases,
from left to right, predictions are made by the GP
(squared exponential kernel),
CNP, NP, ANP respectively.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We see that the GP produces a good fit but could be considered overly
smooth. The NP and CNP both suffer from underfitting which the
ANP resolves by incorporating attention. Both the NP and the ANP are
less smooth than the CNP which could be attributed to the need to take a finite
number of samples of $z$ and $y$ to calculate the mean and variance
of the output.&lt;/p&gt;
&lt;h2 id=&#34;next-time&#34;&gt;Next time&amp;hellip;&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;In this post I gave an overview of how we can learn to model stochastic processes using
probabilistic machine learning models, and specifically through learning the
hyperparameters of a Gaussian process (by maximising the marginal likelihood), or through
optimisation of the parameters of a neural process. This is one of a number of ways that we
can incorporate uncertainty in a principled manner.&lt;/p&gt;
&lt;p&gt;In my next post I plan to delve into another topic entirely, namely reinforcement learning.&lt;/p&gt;
&lt;p&gt;Thanks for reading! Questions and feedback are always much appreciated -
please don&amp;rsquo;t hesitate to get in touch.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;(&lt;a href=&#34;https://www.sciencedirect.com/book/9781483230993/a-first-course-in-stochastic-processes&#34;&gt;Karlin, 1968&lt;/a&gt;): Samuel Karlin. Elements of Stochastic Processes. In A First Course in Stochastic Processes, Elsevier, 1968.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://www-wiley-com.ezp.lib.cam.ac.uk/en-gb/Probability+and+Stochastic+Processes-p-9780470624555&#34;&gt;Florescu, 2014&lt;/a&gt;) Ionut Florescu. Probability and Stochastic Processes. John Wiley &amp;amp; Sons, 2014.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;http://www.gaussianprocess.org/gpml/&#34;&gt;Rasmussen and Williams, 2006&lt;/a&gt;) Carl E. Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning. MIT Press, 2006.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://papers.nips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf&#34;&gt;Snoek, Larochelle and Adams, 2012&lt;/a&gt;)
Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian Optimization of Machine Learning Algorithms. NIPS, 2012.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;http://mlg.eng.cam.ac.uk/pub/pdf/RasKus04.pdf&#34;&gt;Kuss and Rasmussen, 2004&lt;/a&gt;) Malte Kuss and Carl E. Rasmussen.
Gaussian Processes in Reinforcement Learning. Advances in Neural Information Processing Systems, 2004.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://www.jmlr.org/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf&#34;&gt;Quiñonero-Candela and Rasmussen, 2005&lt;/a&gt;)
Joaquin Quiñonero-Candela and Carl E. Rasmussen. A Unifying View of Sparse Approximate Gaussian Process Regression.
Journal of Machine Learning Research, 2005.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf&#34;&gt;Titsias, 2009&lt;/a&gt;) Michalis K. Titsias.
Variational Learning of Inducing Variables in Sparse Gaussian Processes.
Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS), 2009.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/pdf/1807.01613.pdf&#34;&gt;Garnelo et al., 2018a&lt;/a&gt;) Marta Garnelo, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton, Murray Shanahan,
Yee Whye Teh, Danilo J. Rezende, and S. M. Ali Eslami. Conditional Neural Processes.
Proceedings of the 35th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1807.01622&#34;&gt;Garnelo et al., 2018b&lt;/a&gt;) Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola,
Danilo J. Rezende, S. M. Ali Eslami, and Yee Whye Teh.
Neural Processes. ICML 2018 workshop on Theoretical Foundations
and Applications of Deep Generative Models, 2018.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1901.05761&#34;&gt;Kim et al., 2019&lt;/a&gt;) Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami,
Dan Rosenbaum, Oriol Vinyals, and Yee Whye Teh.
Attentive Neural Processes. ICLR, 2019.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Note that this is not the conventional notation for a stochastic process, which usually has
the index parameter labelled $t$, for time, with corresponding random variable $X_t$.
This is because of the usual interpretation of a stochastic process as describing the evolution
of a randomly evolving system over time. Here we breach convention to generalise the index parameter
to an arbitrary number of dimensions. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;In general, we cannot study the joint distribution of an infinite number of random variables (&lt;a href=&#34;https://www-wiley-com.ezp.lib.cam.ac.uk/en-gb/Probability+and+Stochastic+Processes-p-9780470624555&#34;&gt;Florescu, 2014&lt;/a&gt;). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A fully Bayesian approach would specify a prior distribution over model parameters and then
compute the corresponding posterior, which is then integrated over when computing posterior predictive distributions. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The decoder is trying to produce $f(x_i)$. To do so, it needs to have knowledge both of $f$, and the input location $x_i$.
Assuming that $z$ is an adequate summary of $f$, we anticipate that for a flexible enough likelihood,
a forward pass through the decoder will be able to reproduce $f(x_i)$ from $z$ and $x_i$. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Variational Inference</title>
      <link>https://penelopejones.github.io/post/variational_inference/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://penelopejones.github.io/post/variational_inference/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;../bayesian&#34;&gt;last post&lt;/a&gt;, I introduced Bayesian inference as a principled framework for quantifying our level of (un)certainty, in models and predictions. From that post, it can be observed that almost all computations of the quantities of interest
(such as the marginal likelihood) in Bayesian inference demand the integration of complex functions over all parameters of the model of interest.
In practice, these integrals are usually intractable.&lt;/p&gt;
&lt;p&gt;Much work in Bayesian machine learning is focused on developing approximate inference methods
that circumvent the challenge of computing exact integrals. These techniques are generally demarcated as &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) and &lt;em&gt;variational inference&lt;/em&gt; methods.&lt;/p&gt;
&lt;p&gt;MCMC methods are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;computationally expensive,&lt;/li&gt;
&lt;li&gt;scale badly with large amounts of data, and&lt;/li&gt;
&lt;li&gt;it is difficult to assess their convergence, &lt;em&gt;but&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;they are mathematically guaranteed to converge exactly in the limit of infinite data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In contrast, variational inference methods are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;computationally cheaper,&lt;/li&gt;
&lt;li&gt;scale well with large quantities of data, &lt;em&gt;but&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;typically do not converge to the true distribution in the infinite data limit.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;variational-inference&#34;&gt;Variational inference&lt;/h2&gt;
&lt;p&gt;Variational inference (&lt;a href=&#34;https://arxiv.org/abs/1601.00670&#34;&gt;Blei et al., 2017&lt;/a&gt;) methods introduce a variational distribution $q_{\phi}(\theta)$ that upon optimisation of
the variational parameters $\phi$ will be used as an approximation to some true distribution $p(\theta)$.
The power of this family of techniques lies in their ability to simultaneously approximate the posterior distribution
over model parameters and also the marginal likelihood of the model.&lt;/p&gt;
&lt;p&gt;The general approach to VI is as follows.
Suppose that we seek to determine both the marginal likelihood $p(X|M)$ of some model $M$,
and also to determine the posterior over model parameters $\theta$ given the observed data,
$p(\theta|X, M)$.  These two quantities are related by Bayes&amp;rsquo; theorem
$$p(\theta|X, M) = \frac{p(X, \theta|M)}{p(X|M)}.$$
We now introduce the variational distribution $q_{\phi}(\theta)$, which will be used as an approximation to $p(\theta|X, M)$.
Rearranging, and inserting the factor $1 = \frac{q_{\phi}(\theta)}{q_{\phi}(\theta)}$ gives
$$p(X|M) = \frac{p(X, \theta|M)}{q_{\phi}(\theta)}\frac{q_{\phi}(\theta)}{p(\theta|X, M)}.$$
Taking the logarithm of both sides, multiplying by $q_{\phi}(\theta)$ and integrating over $\theta$, we obtain the central relation
$$\log{p(X|M)} = \mathcal{L}_\mathrm{ELBO} + \mathrm{KL}[q_{\phi}(\theta)||p(\theta|X, M)]$$
where the first term is denoted the evidence lower bound (ELBO)
$$\begin{aligned}
\mathcal{L}_\mathrm{ELBO} &amp;amp;= \mathbb{E}_{q_{\phi}(\theta)}\left[\log{\frac{p(X, \theta|M)}{q_{\phi}(\theta)}}\right]  \\&lt;br&gt;
&amp;amp;= \int q_{\phi}(\theta) \log{\frac{p(X, \theta|M)}{q_{\phi}(\theta)}} d\theta,\end{aligned}$$
and the second term
\begin{equation}
\mathrm{KL}[q_{\phi}(\theta)||p(\theta|X, M)] = \int q_{\phi}(\theta) \log{\frac{q_{\phi}(\theta)}{p(\theta|X, M)}} d\theta
\end{equation}
is the Kullback–Leibler (KL) divergence between $q_{\phi}(\theta)$ and $p(\theta|X, M)$
(&lt;a href=&#34;https://projecteuclid.org/euclid.aoms/1177729694&#34;&gt;Kullback and Leibler, 1951&lt;/a&gt;). Crucially,
the KL divergence between two distributions is always non-negative, with equality iff $q_{\phi}(\theta) \equiv p(\theta|X, M)$
(&lt;a href=&#34;https://ieeexplore.ieee.org/document/4218101&#34;&gt;Hershey and Olsen, 2007&lt;/a&gt;).
By minimising the KL divergence (or equivalently, by maximising $\mathcal{L}_\mathrm{ELBO}$) with respect
to variational parameters $\phi$,
we achieve two goals. Firstly, we can use the optimal variational parameters $\phi^*$ to form our approximation to $p(\theta|X, M)$,
using $q^*(\theta) = q_{\phi^*}(\theta)$. Secondly, we will obtain a lower bound and approximation to the marginal likelihood via the
maximised $\mathcal{L}_\mathrm{ELBO}$.
We see that VI turns an intractable integration problem into a tractable optimisation problem which
can be solved using techniques such as stochastic gradient descent.&lt;/p&gt;
&lt;h2 id=&#34;amortised-variational-inference&#34;&gt;Amortised variational inference&lt;/h2&gt;
&lt;p&gt;In contrast to mean field variational inference for which each datapoint has an associated set of variational parameters, in amortised variational inference
the variational parameters are shared across all data points (&lt;a href=&#34;http://gershmanlab.webfactional.com/pubs/GershmanGoodman14.pdf&#34;&gt;Gershman and Goodman, 2014&lt;/a&gt;). Typically
this is achieved using a neural network to learn a mapping from observations to latent variables. The neural network
parameters are the variational parameters to be optimised. The benefit of this is that the number of variational
parameters remains fixed with respect to the number of data points. Also, inference can be performed on unseen data
at the cost of a single forward pass through the so-called `inference network&amp;rsquo;.&lt;/p&gt;
&lt;h2 id=&#34;next-time&#34;&gt;Next time&amp;hellip;&lt;/h2&gt;
&lt;p&gt;In this post, I have provided an overview of variational inference. &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; In my next post I will demonstrate how I
have used variational inference to distil physical insights from soft matter systems.&lt;/p&gt;
&lt;p&gt;Thanks for reading! Feedback always appreciated.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1601.00670&#34;&gt;Blei et al., 2017&lt;/a&gt;) David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statisticians. Journal of the American Statistical Association, 112(518):859–877, April 2017.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://projecteuclid.org/euclid.aoms/1177729694&#34;&gt;Kullback and Leibler, 1951&lt;/a&gt;) Solomon Kullback and Richard .A. Leibler. On Information and Sufficiency. Annals of Mathematical Statistics, 22(1):79–86, March 1951.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://ieeexplore.ieee.org/document/4218101&#34;&gt;Hershey and Olsen, 2007&lt;/a&gt;) John R. Hershey and Peder A. Olsen. Approximating the Kullback-Leibler Divergence Between Gaussian Mixture Models.
In 2007 IEEE International Conference on Acoustics, Speech and Signal Processing– 320, April 2007. ISSN: 2379-190X.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;http://gershmanlab.webfactional.com/pubs/GershmanGoodman14.pdf&#34;&gt;Gershman and Goodman, 2014&lt;/a&gt;) Samuel J. Gershman and Noah D. Goodman.
Amortized Inference in Probabilistic Reasoning.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I plan to cover MCMC methods in a future post! &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Inference</title>
      <link>https://penelopejones.github.io/post/bayesian/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://penelopejones.github.io/post/bayesian/</guid>
      <description>&lt;p&gt;The rise of &lt;em&gt;big data&lt;/em&gt; has been ubiquitous in almost all scientific fields as well as in broader society;
the five most valuable companies in the United States are fundamentally data-driven. &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; It is hardly surprising then,
that media outlets and industrial experts alike have proclaimed that ‘data is the new oil’ in reference to
the mass commoditisation of data globally (&lt;a href=&#34;http://ibisworldwide.com/wp-content/uploads/2015/10/Clive-Humby-RS.pdf&#34;&gt;Humby, 2015&lt;/a&gt;, &lt;a href=&#34;https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data&#34;&gt;The Economist, 2017&lt;/a&gt;, &lt;a href=&#34;https://ieeexplore.ieee.org/document/6863125&#34;&gt;Yi et al., 2014&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The real value in big data has arisen not only from the data itself, but from the concinnity of this
data collection with ever-improving computing power and significant advancements in the tools available
for making sense of the data. Many of these tools have been branded by the umbrella term machine learning.
At the core of these methodologies is the inference of useful insights from our observations, which can be used
to make predictions about unobserved variables, to build models of complex systems and to guide decision-making
processes. In science, machine learning is routinely used in a plethora of applications, from designing novel
molecules for use as medicinal drugs (&lt;a href=&#34;https://www.nature.com/articles/s41557-020-0496-2&#34;&gt;Chodera et al., 2020&lt;/a&gt;), to aiding conservation of coral reefs (&lt;a href=&#34;https://www.nature.com/articles/s42256-020-0192-3&#34;&gt;Nunes et al., 2020&lt;/a&gt;), and, topically, for
forecasting the spread of infectious diseases such as that of the novel coronavirus COVID-19 (&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/30060525/&#34;&gt;Chae et al., 2018&lt;/a&gt;,
&lt;a href=&#34;https://www.pnas.org/content/117/29/16732&#34;&gt;Bertozzi et al., 2020&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;A burgeoning focus of research within machine learning is on the incorporation of probabilistic methodologies
to enable the quantification of uncertainty. This is desirable in many applications of machine learning in science
where it is vital that we ‘know what we do not know’. At the heart of most probabilistic models is the
Bayesian framework which, as we shall see, provides a principled mechanism by which one’s
degree of belief in any event or model can be iteratively updated,
and which enables the quantification of uncertainty in every prediction that one makes.&lt;/p&gt;
&lt;h2 id=&#34;the-omnipresence-of-uncertainty-in-science&#34;&gt;The omnipresence of uncertainty in science&lt;/h2&gt;
&lt;p&gt;A scientist recognises and appreciates the success of prevailing theories and models in explaining her observations, but
is all the while cognisant that no theory should be above scrutiny. The course of history is testament to this world view;
countless theories once considered indubitable truths have been revealed to be flawed. A corollary of this is that no model should ever be considered certain.
Instead, one should assess the extent to which they believe that a model is &lt;em&gt;true&lt;/em&gt; based on the information available to them.&lt;/p&gt;
&lt;p&gt;It is on this premise that the Bayesian view of probability naturally arises. Here, anything about which one is uncertain
is assigned a probability between zero and one, which quantifies one&amp;rsquo;s &lt;em&gt;degree of belief&lt;/em&gt; that the unobserved quantity
will take a specific value. A value close to zero implies a belief that it is unlikely an event will occur. Conversely a
probability close to one implies a belief that an event is likely to occur.&lt;/p&gt;
&lt;p&gt;No aspect of the scientific method is free of uncertainty. When we make a hypothesis using a model, we consciously or
subconsciously assign a prior belief to that hypothesis based on information acquired from previous observations. When
this belief is sufficiently great to warrant our time and efforts, we may test the hypothesis by conducting an experiment.
Inevitably, even our experimental observations are uncertain - our measurements are subject to noise in part due to the
finite precision of our measuring devices. &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; Finally, we use these observations to update our degree of belief in the
model, based on its ability to explain the data.&lt;/p&gt;
&lt;h2 id=&#34;the-bayesian-approach-to-inference&#34;&gt;The Bayesian approach to inference&lt;/h2&gt;
&lt;p&gt;The Bayesian approach to inference provides a principled framework for the quantification of uncertainty at every level.
Using a Bayesian framework, one can iteratively update their beliefs with every new observation.
This continual cycle of making observations and then updating our beliefs based on those observations is conceptually
satisfying, as it is in accordance with our own life experiences.&lt;/p&gt;
&lt;p&gt;The Bayesian paradigm is to update our degree of belief in a model $M$
in light of new data $X$ using Bayes’ theorem,
$$p(M|X) = \frac{p(X|M)p(M)}{p(X)}.$$
Here $p(M)$ is the &lt;em&gt;prior&lt;/em&gt;, which quantifies our degree of belief in
$M$ before observing $X$, and $p(M|X)$ is the &lt;em&gt;posterior&lt;/em&gt;, which
quantifies our degree of belief in $M$ after observing $X$. The
posterior is related to the prior through the &lt;em&gt;likelihood&lt;/em&gt; $p(X|M)$,
which measures how well $M$ can explain $X$, and the marginal likelihood
$p(X)$, which here serves as a normalising constant. When new
observations are made, the previous posterior serves as the new prior,
embodying the iterative nature of learning.&lt;/p&gt;
&lt;p&gt;There are multiple modelling questions that we may wish to answer, all
of which involve performing inference at different levels of detail. For
example, we may want to determine whether data is best explained by a
polynomial function, or by a neural network. Alternatively, we may be
happy to assume a polynomial function can explain the data, but seek to
infer the most probable order of that polynomial function. Finally, at
the lowest level, we may be happy to fix the order of the polynomial,
but wish to infer its coefficients. We can answer all of these questions
using Bayesian inference.&lt;/p&gt;
&lt;p&gt;Let $\mathcal{M}$ denote a family of models. Each member of the family, $M\in\mathcal{M}$, is
defined by a set of hyperparameters and consists of model parameters
$θ_M$. Table 1
provides some common examples of such models and their associated
hyperparameters and parameters.&lt;/p&gt;
&lt;p&gt;Table 1: &lt;em&gt;Examples of models and their hyperparameters and parameters.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model family $\mathcal{M}$&lt;/th&gt;
&lt;th&gt;Defining hyperparameters of model $M\in \mathcal{M}$&lt;/th&gt;
&lt;th&gt;Model parameters $\theta_M$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Polynomial function&lt;/td&gt;
&lt;td&gt;The order, $n$, of the polynomial&lt;/td&gt;
&lt;td&gt;Coefficients $C_n={c_i}_{i=0}^n$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gaussian mixture model&lt;/td&gt;
&lt;td&gt;Number of components&lt;/td&gt;
&lt;td&gt;Means, variances, mixing coefficients&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Neural network&lt;/td&gt;
&lt;td&gt;Number of layers, size of layers, choice of activation function&lt;/td&gt;
&lt;td&gt;Neural network parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The probability of observing a particular dataset is found by
integrating out all other variables from the joint distribution
$$\begin{aligned}
p(X) =&amp;amp; \sum_{\mathcal{M}} \sum_{M} \int p(X, \theta_M, M, \mathcal{M}) d\theta_M \nonumber \\&lt;br&gt;
=&amp;amp; \sum_{\mathcal{M}} p(\mathcal{M}) \sum_{M}p(M|\mathcal{M})\int  p(\theta_M|M, \mathcal{M}) p(X|\theta_M, M, \mathcal{M}) d\theta_M.\end{aligned}$$&lt;/p&gt;
&lt;p&gt;This equation is rather daunting. It is more intuitive to separate the
procedure into three levels. The level(s) of inference that shall be
considered will depend on what the statistician hopes to achieve, and
the extent to which she is willing to restrict the breadth of models
considered.&lt;/p&gt;
&lt;p&gt;At the first level of inference, the model is specified and fixed. The
goal is to determine the posterior $p(\theta_M|X, M, \mathcal{M})$
over the model’s parameters $\theta_M$, given some observed data
$X$. We must specify the prior $p(\theta_M|M, \mathcal{M})$ and the
likelihood $p(X|\theta_M, M, \mathcal{M})$. The posterior is
$$p(\theta_M|X, M, \mathcal{M}) = \frac{p(X|\theta_M, M, \mathcal{M})p(\theta_M|M, \mathcal{M})}{p(X|M, \mathcal{M})}.$$
The normalising constant $p(X|M, \mathcal{M})$
is the &lt;em&gt;marginal likelihood&lt;/em&gt; of the model, and is computed by
integrating over $\theta_M$
$$p(X|M, \mathcal{M}) = \int p(\theta_M| M, \mathcal{M}) p(X|\theta_M, M, \mathcal{M})  d\theta_M.$$&lt;/p&gt;
&lt;p&gt;This quantity is central to the second level of inference, in which the
model family remains fixed, but now we seek to compare the relative
plausibility of two models in the same model family. We can compute the
posterior probability of model $M$ as
$$ p(M|X, \mathcal{M}) = \frac{p(X|M, \mathcal{M})p(M| \mathcal{M})}{p(X|\mathcal{M})},$$&lt;/p&gt;
&lt;p&gt;where $p(X|\mathcal{M})$ is the marginal likelihood of the model &lt;em&gt;family&lt;/em&gt;:
$$p(X|\mathcal{M}) = \sum_{M \in \mathcal{M}} P(X|M, \mathcal{M})p(M|\mathcal{M}).$$&lt;/p&gt;
&lt;p&gt;Assuming a uniform prior over models such that $p(M|\mathcal{M}) \propto 1$, the model
that best explains the data will be that which maximises
$p(X|M, \mathcal{M})$. The marginal likelihood penalises overly complex models
and has an implicit Occam’s razor property: the optimal model will be
the simplest model that can explain the data adequately (&lt;a href=&#34;http://www.inference.org.uk/mackay/thesis.pdf&#34;&gt;MacKay, 1991&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;At the highest level of inference, we could compare the ability of two
different model families to explain $X$. This can be achieved by
computing the posterior distribution of the model family
$$p(\mathcal{M}|X) = \frac{p(X|\mathcal{M})p(\mathcal{M})}{p(X)}.$$
The normalising constant $p(X)$ is independent of $\mathcal{M}$, and typically the
prior over model families $p(\mathcal{M})$ is assumed uniform. &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3 id=&#34;predicting-the-values-of-missing-data&#34;&gt;Predicting the values of missing data&lt;/h3&gt;
&lt;p&gt;Given the observed dataset $X$, we may wish to predict the values of
some unobserved data, $X^{\star}$. In parametric models,
$X^{\star}$ is assumed conditionally independent of $X$ given the
model parameters, such that the distribution
$p(X^{\star}|X, M)$ is
$$p(X^{\star}|X, M) = \int p(X^{\star}|\theta_M, M) p(\theta_M|X, M) d\theta_M.$$&lt;/p&gt;
&lt;h2 id=&#34;next-time&#34;&gt;Next time&amp;hellip;&lt;/h2&gt;
&lt;p&gt;In this post, I outlined the importance of uncertainty quantification in science, and showed how
Bayesian inference can be used to update our degree of belief in measurements and models in light of new observations.&lt;/p&gt;
&lt;p&gt;Next time, I will be covering the basics of variational inference, an important family of techniques for approximating the intractable integrals
that are ubiquitous in Bayesian inference.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;(&lt;a href=&#34;https://www.forbes.com/global2000/#6c50c161335d&#34;&gt;Forbes, 2020&lt;/a&gt;) Global 2000 - The World’s Largest Public Companies 2020, August 2020.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;http://ibisworldwide.com/wp-content/uploads/2015/10/Clive-Humby-RS.pdf&#34;&gt;Humby, 2015&lt;/a&gt;) Clive Humby. Data is the new oil, 2015.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data&#34;&gt;The Economist, 2017&lt;/a&gt;) The world’s most valuable resource is no longer oil, but data. The Economist, May 2017.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://ieeexplore.ieee.org/document/6863125&#34;&gt;Yi et al., 2014&lt;/a&gt;) Xiaomeng Yi, Fangming Liu, Jiangchuan Liu, and Hai Jin. Building a network highway for big data: architecture and challenges. IEEE Network, 28(4):5–13, July 2014. Conference Name: IEEE Network.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://www.nature.com/articles/s41557-020-0496-2&#34;&gt;Chodera et al., 2020&lt;/a&gt;) John Chodera, Alpha A. Lee, Nir London, and Frank von Delft. Crowdsourcing drug discovery for pandemics. Nature Chemistry, 12(7):581–581, July 2020. Number: 7 Publisher: Nature Publishing Group.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://www.nature.com/articles/s42256-020-0192-3&#34;&gt;Nunes et al., 2020&lt;/a&gt;) José Anchieta C. C. Nunes, Igor C. S. Cruz, André Nunes, and Hudson T. Pinheiro. Speeding up coral reef conservation with AI-aided automated image analysis. Nature Machine Intelligence, 2(6):292–292, June 2020. Number: 6 Publisher: Nature Publishing Group.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/30060525/&#34;&gt;Chae et al., 2018&lt;/a&gt;) Sangwon Chae, Sungjun Kwon, and Donghyun Lee. Predicting Infectious Disease Using Deep Learning and Big Data. International Journal of Environmental Research and Public Health, 15(8), August 2018.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://www.pnas.org/content/117/29/16732&#34;&gt;Bertozzi et al., 2020&lt;/a&gt;) Andrea L. Bertozzi, Elisa Franco, George Mohler, Martin B. Short, and Daniel Sledge. The challenges of modeling and forecasting the spread of COVID-19. Proceedings of the National Academy of Sciences, 117(29):16732–16738, July 2020. Publisher: National Academy of Sciences Section: Physical Sciences.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;http://www.inference.org.uk/mackay/thesis.pdf&#34;&gt;MacKay, 1991&lt;/a&gt;) David J.C. MacKay. Bayesian Methods for Adaptive Models. PhD Thesis, December 1991.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;As of August 2020, the five most valuable publicly traded US companies by market capitalisation are Alphabet, Amazon, Apple, Facebook and Microsoft (&lt;a href=&#34;https://www.forbes.com/global2000/#6c50c161335d&#34;&gt;Forbes, 2020&lt;/a&gt;). &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The uncertainty associated with observation noise is termed &lt;em&gt;aleatoric&lt;/em&gt;, as against the &lt;em&gt;epistemic&lt;/em&gt; uncertainty that arises due to a lack of observations. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;This is because there is usually no reason for us to assign a prior
degree of belief to a particular model family. Thus we assume
&lt;em&gt;p&lt;/em&gt;(ℳ) ∝ 1. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>https://penelopejones.github.io/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://penelopejones.github.io/publication/preprint/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>https://penelopejones.github.io/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://penelopejones.github.io/publication/journal-article/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>https://penelopejones.github.io/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://penelopejones.github.io/publication/conference-paper/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
